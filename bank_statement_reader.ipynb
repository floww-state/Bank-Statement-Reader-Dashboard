{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # Find all PDFs from a folder\n",
    "import pdfplumber # To read PDFs\n",
    "import re # To extract important info. using regex\n",
    "import pandas as pd # Needs no explanation :P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PDF data extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractStatement(path: str, possible_date_formats: str, password: str = None, return_non_trans_sentences: bool = False) -> list[str]:\n",
    "    # Returnes a \\n-seperated string with all the date-like lines from a pdf\n",
    "    with pdfplumber.open(path, password=password) as file:\n",
    "        pages = file.pages # Get all pages from PDF\n",
    "        print(f'Reading pdf from \"{path}\".\\n{len(pages)} pages in pdf')\n",
    "        \n",
    "        words = [page.extract_text() for page in pages] # Extract all words from each page\n",
    "        words = '\\n\\n'.join(words) # Convert list of strings from each page to a single string\n",
    "        sentences = words.splitlines() # Convert string to list of sentences\n",
    "        \n",
    "        trans_sentences = [] # Saves transaction-like sentences\n",
    "        non_trans_sentences = [] # Saves non-transaction-like sentences\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            # Check all possible date regexes\n",
    "            date_checks = [bool(re.search(possible_date_format, sentence)) for possible_date_format in possible_date_formats]\n",
    "            is_date = any(date_checks)\n",
    "            # print(is_date)\n",
    "            \n",
    "            if is_date: # If date-like string found\n",
    "                trans_sentences.append(sentence)\n",
    "            \n",
    "            else: # If date-like string not found\n",
    "                non_trans_sentences.append(sentence)\n",
    "        \n",
    "        print(f'{len(trans_sentences)} date-like sentence found (out of {len(sentences)})\\n')\n",
    "        \n",
    "        if return_non_trans_sentences:\n",
    "            return trans_sentences, non_trans_sentences # Return sentences containing date-like values\n",
    "        else:\n",
    "            return trans_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Helper functions - specific information extractor from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractDateFromString(string: str):\n",
    "    # Search for date basis `possible_date_formats` declared earlier\n",
    "    # If match is found w regex at 0, result will be retured and no further search will be done\n",
    "    for regex in possible_date_formats:\n",
    "        result = re.search(regex, string)\n",
    "        \n",
    "        if bool(result):\n",
    "            return result.group()\n",
    "    \n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractAmountFromString(string: str):\n",
    "    # Splits basis ' '\n",
    "    # Reverses the list and which even element can be type casted to float will be returned\n",
    "    values = string.split(' ')\n",
    "    values.reverse()\n",
    "    \n",
    "    for value in values:\n",
    "        value = value.replace(',', '')\n",
    "        \n",
    "        try:\n",
    "            return str(float(value)) # Typecasted to str to avoid e-notation\n",
    "        except ValueError as v:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractCommentsFromString(string: str):\n",
    "    result = re.findall('[a-zA-Z]', string)\n",
    "    result = ''.join(result)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data transformer (data read from text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformStatementV2(transations: list[str]) -> pd.DataFrame:\n",
    "    df = pd.DataFrame(transations, columns=['raw_data'])\n",
    "    \n",
    "    # Normalizing the string-like transation data\n",
    "    df['raw_data'] = df['raw_data'].astype(str).str.strip().str.lower()\n",
    "    \n",
    "    # Identifying debit/credit transations\n",
    "    df['is_credit'] = 0\n",
    "    df.loc[(df['raw_data'].astype(str).str.endswith('cr')), 'is_credit'] = 1\n",
    "    \n",
    "    # Removing 'cr' from raw_data for credit transations\n",
    "    df.loc[(df['is_credit'] == 1), 'raw_data'] = df['raw_data'].str[:-2]\n",
    "    \n",
    "    # Extracting date from raw_data\n",
    "    df['date'] = df['raw_data'].apply(lambda x: extractDateFromString(x))\n",
    "    # df['date'] = pd.to_datetime(df['date'], format='%d/%m/%Y') # TODO: Handle additional formats\n",
    "    \n",
    "    # Extracting amount from raw_data\n",
    "    df['amount'] = df['raw_data'].apply(lambda x: extractAmountFromString(x))\n",
    "    \n",
    "    # Extracting comments from raw_data\n",
    "    df['comments'] = df['raw_data'].astype(str).str.replace('[^a-zA-Z]', ' ', regex=True, case=False)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### All-in-one function executor - from reading PDf to returning structured dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def etl(path: str, password: str, regexes: list):\n",
    "    # print(f'Reading from {path}')\n",
    "    e = extractStatement(path=path, password=password, possible_date_formats=regexes)\n",
    "    t = transformStatementV2(e)\n",
    "    return t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main - algorithm execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG: Pass additional regexes as you deem fit per your data\n",
    "possible_date_formats = [r'([0-9]{2}/[0-9]{2}/[0-9]{4})', r'([0-9]{2}-[a-zA-Z]{3}-[0-9]{2})']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 found in folder `icici_statements`\n"
     ]
    }
   ],
   "source": [
    "# Pass folder path here\n",
    "folder_path = r'D:\\work\\code_and_stuff\\git_repos\\bank_statement_parser\\Bank-Statement-Reader-Dashboard\\1_data\\icici_statements'\n",
    "file_paths = [folder_path + '\\\\' + file for file in os.listdir(folder_path)]\n",
    "print(f'{len(file_paths)} found in folder `{folder_path.split('\\\\')[-1]}`')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading pdf from \"D:\\work\\code_and_stuff\\git_repos\\bank_statement_parser\\Bank-Statement-Reader-Dashboard\\1_data\\icici_statements\\2024-23-11--19-13-26-creditAnnualStmt.pdf\".\n",
      "15 pages in pdf\n",
      "353 date-like sentence found (out of 616)\n",
      "\n",
      "File read: 1\n",
      "# of Transactions: (353, 5)\n",
      "\n",
      " File saved successfully!\n"
     ]
    }
   ],
   "source": [
    "bank_name = 'ICICI' # Pass bank name\n",
    "output_path = r'D:\\work\\code_and_stuff\\git_repos\\bank_statement_parser\\Bank-Statement-Reader-Dashboard\\2_output'\n",
    "\n",
    "dfs = [etl(file_path, None, possible_date_formats) for file_path in file_paths] # Executes all the required functions to get data from PDF to padnas DF\n",
    "print(f'File read: {len(dfs)}')\n",
    "\n",
    "df = pd.concat(dfs, axis=0, ignore_index=True) # Each file will have own DF, we'll concat all DFs\n",
    "print(f'# of Transactions: {df.shape}')\n",
    "\n",
    "df['bank'] = bank_name # Adding bank name to easily track source\n",
    "df['source'] = folder_path # Adding folder path where PDFs were read\n",
    "\n",
    "#TODO: Add criteria to identify junk transactions and remove them from output\n",
    "\n",
    "os.makedirs(output_path, exist_ok=True) # Makes folder where output will be stored\n",
    "df.to_csv(output_path + '\\\\' + f'processed_{bank_name}.csv', index=False) # Stored the processed and structured data in a csv file\n",
    "print('\\n', 'File saved successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
